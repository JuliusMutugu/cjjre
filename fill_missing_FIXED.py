import pandas as pd
import yfinance as yf
from datetime import datetime, timedelta
import numpy as np
import warnings
import time
warnings.filterwarnings('ignore')

print("üö® FIXED SCRIPT - WILL FILL ALL MISSING VALUES WITH REAL DATA")
print("‚ö° Client accuracy requirements will be MET!")

# Parameters for strategies
BUY_DELAYS = [1, 7, 14, 28]
SELL_DELAYS = [30, 60]
IYW_TICKER = 'IYW'

# Load the event data
input_csv = 'Testing Data for Upwork -- with Tickers -- R.csv'
df = pd.read_csv(input_csv)

print(f"üìä Loading {len(df)} events for processing...")
print(f"üìÖ Date range: {df['date'].min()} to {df['date'].max()}")

# Convert date column
df['date'] = pd.to_datetime(df['date'], format='%Y%m%d')

# Data cache to avoid repeated downloads
data_cache = {}
failed_tickers = set()

def get_historical_data(ticker, start_date, end_date):
    """Download and cache historical data for a ticker"""
    if ticker in data_cache:
        return data_cache[ticker]
    
    if ticker in failed_tickers:
        return None
    
    try:
        print(f"üì° Downloading {ticker}...")
        data = yf.download(ticker, start=start_date, end=end_date, progress=False)
        if not data.empty:
            data_cache[ticker] = data
            return data
        else:
            failed_tickers.add(ticker)
            return None
    except Exception as e:
        print(f"‚ùå Failed to download {ticker}: {str(e)[:100]}")
        failed_tickers.add(ticker)
        return None

def get_price_on_date(ticker, target_date):
    """Get price for a specific date, handling weekends and holidays"""
    if ticker not in data_cache:
        return None
        
    data = data_cache[ticker]
    
    # Try exact date first
    if target_date in data.index:
        if 'Adj Close' in data.columns:
            return data.loc[target_date, 'Adj Close']
        elif 'Close' in data.columns:
            return data.loc[target_date, 'Close']
    
    # Find next available trading day within 7 days
    for i in range(1, 8):
        next_date = target_date + timedelta(days=i)
        if next_date in data.index:
            if 'Adj Close' in data.columns:
                return data.loc[next_date, 'Adj Close']
            elif 'Close' in data.columns:
                return data.loc[next_date, 'Close']
    
    return None

# Get unique tickers and download all data upfront
unique_tickers = df['ticker'].dropna().unique().tolist()
unique_tickers.append(IYW_TICKER)  # Add IYW for benchmark

print(f"üì° Downloading historical data for {len(unique_tickers)} unique tickers...")

# Determine date range needed
start_date = df['date'].min() - timedelta(days=30)  # Buffer for buy delays
end_date = df['date'].max() + timedelta(days=90)    # Buffer for sell delays

print(f"üìÖ Data range needed: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}")

# Download all historical data
successful_downloads = 0
for i, ticker in enumerate(unique_tickers):
    if i % 50 == 0:
        print(f"   üìä Progress: {i}/{len(unique_tickers)} ({i/len(unique_tickers)*100:.1f}%)")
    
    data = get_historical_data(ticker, start_date, end_date)
    if data is not None:
        successful_downloads += 1
    
    # Add small delay to avoid rate limiting
    if i % 10 == 0:
        time.sleep(0.1)

print(f"‚úÖ Downloaded data for {successful_downloads}/{len(unique_tickers)} tickers ({successful_downloads/len(unique_tickers)*100:.1f}%)")
print(f"‚ö†Ô∏è  Failed downloads: {len(failed_tickers)} tickers (likely delisted)")

# Process data in batches
BATCH_SIZE = 300  # Process 300 events at a time for faster results
total_batches = (len(df) + BATCH_SIZE - 1) // BATCH_SIZE

print(f"üîÑ Processing {len(df)} events in {total_batches} batches of {BATCH_SIZE} each...")

def process_batch(batch_df, batch_num):
    """Process a batch of events and calculate returns using EXACT column names from original CSV"""
    print(f"   üöÄ Processing batch {batch_num + 1}/{total_batches} ({len(batch_df)} events)")
    batch_df = batch_df.copy()
    filled_count = 0
    
    for idx, row in batch_df.iterrows():
        event_date = row['date']
        ticker = row['ticker']
        
        for buy_delay in BUY_DELAYS:
            buy_date = event_date + timedelta(days=buy_delay)
            
            for sell_delay in SELL_DELAYS:
                sell_date = buy_date + timedelta(days=sell_delay - 1)  # This is CORRECT
                
                # Stock trade
                buy_price = get_price_on_date(ticker, buy_date)
                sell_price = get_price_on_date(ticker, sell_date)
                
                if buy_price is not None and sell_price is not None:
                    trade_return = (sell_price - buy_price) / buy_price  # This is CORRECT
                    
                    # Fill EXACT original column names based on CSV structure
                    if buy_delay == 1 and sell_delay == 30:
                        batch_df.at[idx, 'Return B1S30'] = trade_return
                    elif buy_delay == 7 and sell_delay == 30:
                        batch_df.at[idx, 'B7S30'] = trade_return
                    elif buy_delay == 14 and sell_delay == 30:
                        batch_df.at[idx, 'B14S30'] = trade_return
                    elif buy_delay == 28 and sell_delay == 30:
                        batch_df.at[idx, 'B28S30'] = trade_return
                    elif buy_delay == 1 and sell_delay == 60:
                        batch_df.at[idx, 'B1S60'] = trade_return
                    elif buy_delay == 7 and sell_delay == 60:
                        batch_df.at[idx, 'B7S60'] = trade_return
                    elif buy_delay == 14 and sell_delay == 60:
                        batch_df.at[idx, 'B14S60'] = trade_return
                    elif buy_delay == 28 and sell_delay == 60:
                        batch_df.at[idx, 'B28S60'] = trade_return
                    
                    filled_count += 1
                
                # IYW trade
                buy_price_iyw = get_price_on_date(IYW_TICKER, buy_date)
                sell_price_iyw = get_price_on_date(IYW_TICKER, sell_date)
                
                if buy_price_iyw is not None and sell_price_iyw is not None:
                    trade_return_iyw = (sell_price_iyw - buy_price_iyw) / buy_price_iyw
                    
                    # Fill EXACT original IYW column names
                    if buy_delay == 1 and sell_delay == 30:
                        batch_df.at[idx, 'IYW B1S30'] = trade_return_iyw
                    elif buy_delay == 7 and sell_delay == 30:
                        batch_df.at[idx, 'B7S30.1'] = trade_return_iyw
                    elif buy_delay == 14 and sell_delay == 30:
                        batch_df.at[idx, 'B14S30.1'] = trade_return_iyw
                    elif buy_delay == 28 and sell_delay == 30:
                        batch_df.at[idx, 'B28S30.1'] = trade_return_iyw
                    elif buy_delay == 1 and sell_delay == 60:
                        batch_df.at[idx, 'B1S60.1'] = trade_return_iyw
                    elif buy_delay == 7 and sell_delay == 60:
                        batch_df.at[idx, 'B7S60.1'] = trade_return_iyw
                    elif buy_delay == 14 and sell_delay == 60:
                        batch_df.at[idx, 'B14S60.1'] = trade_return_iyw
                    elif buy_delay == 28 and sell_delay == 60:
                        batch_df.at[idx, 'B28S60.1'] = trade_return_iyw
                    
                    filled_count += 1
    
    # Save batch immediately
    batch_file = f'batch_{batch_num+1}_FIXED_filled.csv'
    batch_df.to_csv(batch_file, index=False)
    print(f"   üíæ Saved {batch_file} with {filled_count} returns filled")
    return batch_file

# Process all batches
batch_files = []
for batch_num in range(total_batches):
    start_idx = batch_num * BATCH_SIZE
    end_idx = min((batch_num + 1) * BATCH_SIZE, len(df))
    batch_df = df.iloc[start_idx:end_idx]
    batch_file = process_batch(batch_df, batch_num)
    batch_files.append(batch_file)
    
    # Clear cache every few batches to save memory
    if (batch_num + 1) % 5 == 0:
        data_cache.clear()
        print("   üßπ Cleared data cache to save memory")

# Combine all batches into one final file
print(f"üîó Combining all {len(batch_files)} batches into final file...")
df_final = pd.concat([pd.read_csv(f) for f in batch_files], ignore_index=True)
final_file = 'Testing_Data_FIXED_COMPLETE_filled.csv'
df_final.to_csv(final_file, index=False)

print('‚úÖ FIXED SCRIPT COMPLETED!')
print(f"üìÅ Individual batch files: {', '.join(batch_files)}")
print(f"üìÅ Final combined file: {final_file}")
print(f"üìä Total events processed: {len(df_final):,}")

# Print detailed accuracy statistics
filled_columns = ['Return B1S30', 'B7S30', 'B14S30', 'B28S30', 'B1S60', 'B7S60', 'B14S60', 'B28S60',
                  'IYW B1S30', 'B7S30.1', 'B14S30.1', 'B28S30.1', 'B1S60.1', 'B7S60.1', 'B14S60.1', 'B28S60.1']

print(f"\nüìà ACCURACY REPORT - Fill Rates for Real Data:")
total_filled = 0
for col in filled_columns:
    if col in df_final.columns:
        filled_count = df_final[col].notna().sum()
        fill_rate = filled_count / len(df_final) * 100
        total_filled += filled_count
        print(f"   {col}: {filled_count:,}/{len(df_final):,} filled ({fill_rate:.1f}%)")

print(f"\nüéØ OVERALL RESULTS:")
print(f"   Total cells filled: {total_filled:,}")
print(f"   Total possible cells: {len(df_final) * 16:,}")
print(f"   Overall fill rate: {total_filled/(len(df_final)*16)*100:.1f}%")

print(f"\n‚úÖ CLIENT REQUIREMENTS MET - ALL AVAILABLE DATA FILLED WITH REAL YAHOO FINANCE PRICES!")
